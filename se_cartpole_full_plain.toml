[setup]

device = ["cuda:0"]
use_mlflow = true
mlflow_run_name = "Full-Plain-New reward metrics"

mlflow_uri = "http://10.30.20.11:5000"
mlflow_experiment = "jheis_SRL_Cartpole"


[parameters] # these are logged in mlflow

# environment
env = {id = "CartPole-v1", max_episode_steps = 500}
num_envs = 11
truncation_is_termination = true

# model architecture (excluding input & output size since these are determined by the environment)
# because TOML doesn't support heterogeneous arrays, all entries must be strings, which are passed through eval()
architecture_hidden = ["100", "100"]

# training lengths
n_episodes = 200
n_steps_per_episode = 500
n_train_epochs = 32 # training cycles per epoch
batch_size = 128

# model parameters
model_class = "ScaledRewardLearner"
target_update = 0.85
# target_update = 20
use_reward_scaling = false

# variable parameters
gamma = {fn_type = "constant", c = 0.99}                                            # discount factor
epsilon = {fn_type = "logistic", start = 1, end = 0.05, midpoint = 30, rate = 0.1}  # chance for random action
learning_rate = {fn_type = "exponential", start = 3e-4, factor = 0.9997}            # learning rate

# replay buffer
replay_buffer_size = 100_000

# save parameters
# start_episode = 0
save_interval = 10
save_path = "weights/cartpole/full_plain"

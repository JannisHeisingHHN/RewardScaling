{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096eab50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as tc\n",
    "from torch import nn\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "from agents import *\n",
    "from train import obs_to_state\n",
    "\n",
    "import mlflow\n",
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DEVICE = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee3fa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select model\n",
    "Model: type[Learner] = ScaledRewardLearner\n",
    "# path_weight = \"weights/full_plain/\"\n",
    "# path_weight = \"weights/test/\"\n",
    "path_weight = \"weights/lander_full_plain/\"\n",
    "\n",
    "# select environment (action space has to be discrete)\n",
    "\n",
    "# env_params = {'id': \"MountainCar-v0\"}\n",
    "\n",
    "# gym.register(\"TestEnv\", \"envs.test_env:TestEnv\")\n",
    "# env_params = {'id': \"TestEnv\", 'target': 300, 'radius': 5}\n",
    "\n",
    "env_params = {'id': \"LunarLander-v2\"}\n",
    "\n",
    "# select settings\n",
    "EPOCH = 7000\n",
    "SHOW_WINDOW = True\n",
    "N_STEPS = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769272e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create environment\n",
    "if SHOW_WINDOW:\n",
    "    env_vis = gym.make(**env_params, max_episode_steps=N_STEPS, render_mode=\"human\")\n",
    "else:\n",
    "    env_vis = gym.make(**env_params, max_episode_steps=N_STEPS)\n",
    "\n",
    "# load agent from checkpoint\n",
    "with tc.serialization.safe_globals([nn.BatchNorm1d]):\n",
    "    agent_vis = Model.load(path_weight, EPOCH, DEVICE)\n",
    "agent_vis.eval()\n",
    "\n",
    "# create action lookup\n",
    "n_actions = int(env_vis.action_space.n) # type: ignore\n",
    "actions_onehot = tc.eye(n_actions, dtype=tc.int, device=agent_vis.device)\n",
    "\n",
    "# keep track of states & actions\n",
    "states = []\n",
    "actions = []\n",
    "\n",
    "# reset environment\n",
    "observation, info = env_vis.reset() \n",
    "\n",
    "\n",
    "tqdm_iter = trange(N_STEPS)\n",
    "for t in tqdm_iter:\n",
    "    with tc.no_grad():\n",
    "        state = obs_to_state(observation, DEVICE).unsqueeze(0)\n",
    "\n",
    "        action = agent_vis.act(state, actions_onehot).squeeze()\n",
    "\n",
    "    # perform step\n",
    "    observation, reward, terminated, truncated, info = env_vis.step(action)\n",
    "\n",
    "    # store state & action\n",
    "    states.append(state)\n",
    "    actions.append(float(action))\n",
    "\n",
    "    # update progress bar title\n",
    "    tqdm_iter.set_description(f\"Chose action {action}\")\n",
    "\n",
    "    # If the episode has ended then we can reset to start a new episode\n",
    "    if terminated or truncated:\n",
    "        observation, info = env_vis.reset()\n",
    "\n",
    "env_vis.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddb0bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_vis.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2367bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(actions, bins=50)\n",
    "plt.xticks()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e067dff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(states)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e26c99",
   "metadata": {},
   "source": [
    "# Metric retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fd5852",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"http://10.30.20.11:5000\")\n",
    "mlflow.set_experiment(\"jheis_SRL_MountainCar-v0\")\n",
    "\n",
    "client = mlflow.tracking.MlflowClient() # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef54fecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = \"f70604c2f06147de9d7577b04016208a\"\n",
    "\n",
    "get_history = lambda name: np.array(list(map(lambda x: x.value, client.get_metric_history(run_id, name))))\n",
    "\n",
    "history_lq1 = get_history(\"loss_q1\")\n",
    "history_lq2 = get_history(\"loss_q2\")\n",
    "history_el = get_history(\"episode_length\")\n",
    "history_q = get_history(\"q\")\n",
    "history_reward = get_history(\"mean_reward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66249b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 5))\n",
    "\n",
    "xmin = 0\n",
    "xmax = len(history_q) - 1\n",
    "plt.hlines([0], xmin, xmax, colors=\"black\")\n",
    "# plt.plot(history_lq1, label=\"Q loss\")\n",
    "plt.plot(history_q, label=\"Q value\")\n",
    "plt.plot(history_reward, label=\"Reward\")\n",
    "# plt.plot(history_el > 200)\n",
    "# plt.plot(history_reward)\n",
    "# plt.semilogy()\n",
    "# plt.ylim(38, 40)\n",
    "\n",
    "plt.xlim(xmin, xmax)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.title(\"SAC training metrics\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af709e3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gymnasium",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

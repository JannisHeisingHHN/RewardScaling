{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096eab50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as tc\n",
    "from torch import nn\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "from scaled_reward_learner import ScaledRewardLearner, _obs_to_state\n",
    "from replay_buffer import ReplayBuffer\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "import mlflow.pytorch as mlflow_tc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "DEVICE = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee3fa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters mostly taken from the original SAC paper\n",
    "params = {\n",
    "    # environment\n",
    "    'env': {\n",
    "        'id': \"MountainCar-v0\",\n",
    "        'max_episode_steps': 1000,\n",
    "    },\n",
    "\n",
    "    # model architecture (excluding input & output size since these are determined by the environment)\n",
    "    'architecture_policy_hidden': [256, nn.BatchNorm1d(256), 256],\n",
    "}\n",
    "\n",
    "EPOCH = 0\n",
    "SHOW_WINDOW = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769272e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell is adapted from https://gymnasium.farama.org/\n",
    "if SHOW_WINDOW:\n",
    "    env_vis = gym.make(**params['env'], render_mode=\"human\")\n",
    "else:\n",
    "    env_vis = gym.make(**params['env'])\n",
    "\n",
    "# load agent from checkpoint\n",
    "state_size = env_vis.observation_space.shape[-1] # type: ignore\n",
    "action_size = int(env_vis.action_space.n) # type: ignore\n",
    "\n",
    "agent_vis = ScaledRewardLearner.load(\"weights/srl_mountaincar\", EPOCH, DEVICE)\n",
    "agent_vis.eval()\n",
    "\n",
    "# keep track of actions\n",
    "actions = []\n",
    "\n",
    "# Reset the environment to generate the first observation\n",
    "observation, info = env_vis.reset(seed=42) \n",
    "for t in trange(1000):\n",
    "    # this is where you would insert your policy\n",
    "    with tc.no_grad():\n",
    "        state = _obs_to_state(observation, DEVICE).unsqueeze(0)\n",
    "\n",
    "        action = agent_vis.act(state).squeeze()\n",
    "        # action = np.array([-2])\n",
    "\n",
    "    # step (transition) through the environment with the action\n",
    "    # receiving the next observation, reward and if the episode has terminated or truncated\n",
    "    observation, reward, terminated, truncated, info = env_vis.step(action)\n",
    "\n",
    "    # store action\n",
    "    actions.append(float(action))\n",
    "\n",
    "    # If the episode has ended then we can reset to start a new episode\n",
    "    if terminated or truncated:\n",
    "        observation, info = env_vis.reset()\n",
    "\n",
    "env_vis.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970309a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_vis.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2367bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(actions, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e26c99",
   "metadata": {},
   "source": [
    "# Metric retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fd5852",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"http://127.0.0.1:8080\")\n",
    "mlflow.set_experiment(\"MountainCar-v0\")\n",
    "\n",
    "client = mlflow.tracking.MlflowClient() # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef54fecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = \"\"\n",
    "\n",
    "get_history = lambda name: np.array(list(map(lambda x: x.value, client.get_metric_history(run_id, name))))\n",
    "\n",
    "history_lq1 = get_history(\"loss_q1\")\n",
    "history_lq2 = get_history(\"loss_q2\")\n",
    "history_el = get_history(\"episode_length\")\n",
    "history_q = get_history(\"q\")\n",
    "history_reward = get_history(\"mean_reward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66249b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 5))\n",
    "\n",
    "xmin = 0\n",
    "xmax = len(history_q) - 1\n",
    "plt.hlines([0], xmin, xmax, colors=\"black\")\n",
    "plt.plot(history_lq1, label=\"Q loss\")\n",
    "plt.plot(history_q, label=\"Q value\")\n",
    "plt.plot(history_reward, label=\"Reward\")\n",
    "# plt.plot(history_el > 200)\n",
    "# plt.plot(history_reward)\n",
    "# plt.semilogy()\n",
    "# plt.ylim(38, 40)\n",
    "\n",
    "plt.xlim(xmin, xmax)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.title(\"SAC training metrics\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af709e3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gymnasium",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
